{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Dict, List\n",
    "from pathlib import Path\n",
    "import json\n",
    "from importlib import import_module\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import recall_score, precision_score, precision_recall_curve\n",
    "\n",
    "from src.model import StochasticModel, SimpleFastTextClassifier, SimpleAudioClassifier, ComplicatedAudioClassifier, ResNet, AST\n",
    "from utils.metrics import binary_weighted_accuracy\n",
    "from utils.parameters import SEED, LABELS_NAMES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/CMU-MOSEI\")\n",
    "LABELS_DIR = DATA_DIR / \"Labels\"\n",
    "AUDIO_DIR = DATA_DIR / \"Audio_chunk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod_csv = pd.read_csv(LABELS_DIR / \"Data_Train_modified.csv\")\n",
    "val_mod_csv = pd.read_csv(LABELS_DIR / \"Data_Val_modified.csv\")\n",
    "test_mod_csv = pd.read_csv(LABELS_DIR / \"Data_Test_modified.csv\")\n",
    "\n",
    "val_orig_csv = pd.read_csv(LABELS_DIR / \"Data_Val_original_without_neg_time.csv\")\n",
    "test_orig_csv = pd.read_csv(LABELS_DIR / \"Data_Test_original_without_neg_time.csv\")\n",
    "\n",
    "\n",
    "train_info = None\n",
    "for label_name in LABELS_NAMES:\n",
    "    train_mod_csv.loc[train_mod_csv[label_name] > 0, label_name] = 1\n",
    "    train_mod_csv[label_name] = train_mod_csv[label_name].astype(int)\n",
    "    train_info = pd.concat([train_info, train_mod_csv[label_name].value_counts()], axis=1)\n",
    "\n",
    "val_info = None\n",
    "for label_name in LABELS_NAMES:\n",
    "    val_mod_csv.loc[val_mod_csv[label_name] > 0, label_name] = 1\n",
    "    val_mod_csv[label_name] = val_mod_csv[label_name].astype(int)\n",
    "    val_info = pd.concat([val_info, val_mod_csv[label_name].value_counts()], axis=1)\n",
    "\n",
    "test_info = None\n",
    "for label_name in LABELS_NAMES:\n",
    "    test_orig_csv.loc[test_orig_csv[label_name] > 0, label_name] = 1\n",
    "    test_orig_csv[label_name] = test_orig_csv[label_name].astype(int)\n",
    "    test_info = pd.concat([test_info, test_orig_csv[label_name].value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(csv: Path, dir_name: str):\n",
    "    csv = csv[[\"video\", \"start_time\", \"end_time\"]]\n",
    "    markuped_features_dir = Path(\"data/CMU-MOSEI/fasttext_featrues_markuped_text\")\n",
    "    recognised_features_dir = Path(\"data/CMU-MOSEI/fasttext_featrues_recognised_text\")\n",
    "    features: List[np.ndarray] = list()\n",
    "\n",
    "    progress_bar = tqdm(csv.values, desc=\"Features loading\")\n",
    "    for ytid, start_time, end_time in progress_bar:\n",
    "        file_name = f\"{ytid}_{float(start_time):.4f}_{float(end_time):.4f}.npy\"\n",
    "        try:\n",
    "            markuped_feature = np.load(markuped_features_dir / dir_name / file_name)\n",
    "        except FileNotFoundError as exception:\n",
    "            markuped_feature = np.zeros(300)\n",
    "            print(exception)\n",
    "        try:\n",
    "            recognised_feature = np.load(recognised_features_dir / dir_name / file_name)\n",
    "        except FileNotFoundError as exception:\n",
    "            recognised_feature = np.zeros(300)\n",
    "            print(exception)\n",
    "        feature = np.concatenate((markuped_feature, recognised_feature), axis=0)\n",
    "        features.append(torch.from_numpy(feature))\n",
    "\n",
    "    return torch.stack(features).to(torch.float32)\n",
    "\n",
    "def get_audio_features(csv: Path, dir_name: str):\n",
    "    csv = csv[[\"video\", \"start_time\", \"end_time\"]]\n",
    "    features_dir = Path(\"data/CMU-MOSEI/audio_featrues\")\n",
    "    features: List[torch.Tensor] = list()\n",
    "\n",
    "    progress_bar = tqdm(csv.values, desc=\"Features loading\")\n",
    "    for ytid, start_time, end_time in progress_bar:\n",
    "        file_name = f\"{ytid}_{float(start_time):.4f}_{float(end_time):.4f}.pt\"\n",
    "        try:\n",
    "            feature = torch.load(features_dir / dir_name / file_name)\n",
    "            features.append(feature)\n",
    "        except FileNotFoundError as exception:\n",
    "            print(exception)\n",
    "    return torch.stack(features).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba12f285fab42348384a5aeb0f40d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Features loading:   0%|          | 0/1861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74f94db028d4434ba722cc8d27b800c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Features loading:   0%|          | 0/1861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_targets = torch.tensor(val_mod_csv[LABELS_NAMES].values, dtype=torch.float32)\n",
    "val_text_features = get_text_features(val_mod_csv, \"val_modified\")\n",
    "val_audio_features = get_audio_features(val_mod_csv, \"val_modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081ed8dfbdc244a48d54fd3f2b3c9180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Features loading:   0%|          | 0/4662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'data/CMU-MOSEI/fasttext_featrues_recognised_text/test_original/180971_0.0000_0.5720.npy'\n",
      "[Errno 2] No such file or directory: 'data/CMU-MOSEI/fasttext_featrues_recognised_text/test_original/194299_0.0000_0.5920.npy'\n",
      "[Errno 2] No such file or directory: 'data/CMU-MOSEI/fasttext_featrues_recognised_text/test_original/267466_0.0000_0.6520.npy'\n",
      "[Errno 2] No such file or directory: 'data/CMU-MOSEI/fasttext_featrues_recognised_text/test_original/46495_0.0000_0.6620.npy'\n",
      "[Errno 2] No such file or directory: 'data/CMU-MOSEI/fasttext_featrues_recognised_text/test_original/DjcZrtcBZi4_0.0000_1.7297.npy'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ae96cdeefd4983b5b7ad696b1fe237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Features loading:   0%|          | 0/4662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_targets = torch.tensor(test_orig_csv[LABELS_NAMES].values, dtype=torch.float32)\n",
    "test_text_features = get_text_features(test_orig_csv, \"test_original\")\n",
    "test_audio_features = get_audio_features(test_orig_csv, \"test_original\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path: str, checkpoint_path: str, device: str):\n",
    "    config: Dict[str, Dict[str, Any]] = json.loads(Path(config_path).read_text())\n",
    "                                                   \n",
    "    module = import_module(config[\"model\"][\"source\"])\n",
    "    ModelClass = getattr(module, config[\"model\"][\"name\"])\n",
    "    model: nn.Module = ModelClass(**config[\"model\"][\"prams\"]).to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sazerlife/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "SimpleFastTextClassifier_config_path = \"experiments/fasttext_classifier/exp2-simple-model-weighted/config.json\"\n",
    "SimpleFastTextClassifier_checkpoint_path = \"experiments/fasttext_classifier/exp2-simple-model-weighted/checkpoints/best_model-36500.pt\"\n",
    "ComplicatedFastTextClassifier_config_path = \"experiments/fasttext_classifier/exp3-complicated-model-weighted/config.json\"\n",
    "ComplicatedFastTextClassifier_checkpoint_path = \"experiments/fasttext_classifier/exp3-complicated-model-weighted/checkpoints/best_model-28000.pt\"\n",
    "\n",
    "SimpleAudioClassifier_config_path = \"experiments/audio_classifier/exp1-simple-model-weighted/config.json\"\n",
    "SimpleAudioClassifier_checkpoint_path = \"experiments/audio_classifier/exp1-simple-model-weighted/checkpoints/best_model-25.pt\"\n",
    "ComplicatedAudioClassifier_config_path = \"experiments/audio_classifier/exp2-complicated-model-weighted/config.json\"\n",
    "ComplicatedAudioClassifier_checkpoint_path = \"experiments/audio_classifier/exp2-complicated-model-weighted/checkpoints/250.pt\"\n",
    "ResNet_config_path = \"experiments/audio_classifier/exp3-resnet-weighted/config.json\"\n",
    "ResNet_checkpoint_path = \"experiments/audio_classifier/exp3-resnet-weighted/checkpoints/best_model-48.pt\"\n",
    "AST_config_path = \"experiments/audio_classifier/exp4-AST-weighted/config.json\"\n",
    "AST_checkpoint_path = \"experiments/audio_classifier/exp4-AST-weighted/checkpoints/16.pt\"\n",
    "\n",
    "SimpleMultimodalClassifier_config_path = \"experiments/multimodal_classifier/exp1-simple-model-weighted/config.json\"\n",
    "# SimpleMultimodalClassifier_checkpoint_path = \"experiments/multimodal_classifier/exp1-simple-model-weighted/checkpoints/best_model-105.pt\"\n",
    "SimpleMultimodalClassifier_checkpoint_path = \"experiments/multimodal_classifier/exp1-simple-model-weighted/checkpoints/960.pt\"\n",
    "\n",
    "models: Dict[str, nn.Module] = {\n",
    "    \"Stochastic Model\": StochasticModel(train_mod_csv).eval().to(DEVICE),\n",
    "    \n",
    "    \"Simple FastText Classifier\": load_model(SimpleFastTextClassifier_config_path, SimpleFastTextClassifier_checkpoint_path, DEVICE),\n",
    "    \"Complicated FastText Classifier\": load_model(ComplicatedFastTextClassifier_config_path, ComplicatedFastTextClassifier_checkpoint_path, DEVICE),\n",
    "    \n",
    "    \"Simple Audio Classifier\": load_model(SimpleAudioClassifier_config_path, SimpleAudioClassifier_checkpoint_path, DEVICE),\n",
    "    \"Complicated Audio Classifier\": load_model(ComplicatedAudioClassifier_config_path, ComplicatedAudioClassifier_checkpoint_path, DEVICE),\n",
    "    \"ResNet\": load_model(ResNet_config_path, ResNet_checkpoint_path, DEVICE),\n",
    "    \"AST\": load_model(AST_config_path, AST_checkpoint_path, DEVICE),\n",
    "\n",
    "    \"Simple Multimodal Classifier\": load_model(SimpleMultimodalClassifier_config_path, SimpleMultimodalClassifier_checkpoint_path, DEVICE),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = Path(\"experiments/fasttext_classifier/exp2-simple-model-weighted/config.json\")\n",
    "# simple_fasttext_classifier_config: Dict[str, Dict[str, Any]] = json.loads(config_path.read_text())\n",
    "# config_path = Path(\"experiments/audio_classifier/exp1-simple-model-weighted/config.json\")\n",
    "# simple_audio_classifier_config: Dict[str, Dict[str, Any]] = json.loads(config_path.read_text())\n",
    "\n",
    "# config_path = Path(\"experiments/audio_classifier/exp2-complicated-model-weighted/config.json\")\n",
    "# complicated_audio_classifier_config: Dict[str, Dict[str, Any]] = json.loads(config_path.read_text())\n",
    "\n",
    "# models: Dict[str, nn.Module] = {\n",
    "#     \"Stochastic Model\": StochasticModel(train_mod_csv).eval().to(DEVICE),\n",
    "    \n",
    "#     \"Simple FastText Classifier\": SimpleFastTextClassifier(**simple_fasttext_classifier_config[\"model\"][\"prams\"]).eval().to(DEVICE),\n",
    "#     \"Simple Audio Classifier\": SimpleAudioClassifier(**simple_audio_classifier_config[\"model\"][\"prams\"]).eval().to(DEVICE),\n",
    "    \n",
    "#     \"Complicated Audio Classifier\": ComplicatedAudioClassifier(**complicated_audio_classifier_config[\"model\"][\"prams\"]).eval().to(DEVICE),\n",
    "#     \"ResNet\": ResNet(**simple_audio_classifier_config[\"model\"][\"prams\"]).eval().to(DEVICE),\n",
    "#     \"AST\": AST(**simple_audio_classifier_config[\"model\"][\"prams\"]).eval().to(DEVICE),\n",
    "# }\n",
    "\n",
    "# simple_fasttext_classifier_checkpoint = torch.load(\"experiments/fasttext_classifier/exp2-simple-model-weighted/checkpoints/best_model-36500.pt\")\n",
    "# models[\"Simple FastText Classifier\"].load_state_dict(simple_fasttext_classifier_checkpoint[\"state_dict\"])\n",
    "\n",
    "# simple_audio_classifier_checkpoint = torch.load(\"experiments/audio_classifier/exp1-simple-model-weighted/checkpoints/best_model-25.pt\")\n",
    "# models[\"Simple Audio Classifier\"].load_state_dict(simple_audio_classifier_checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(fetures: torch.Tensor, model: nn.Module, threshold: int, device: str, batch_size: int = 512):\n",
    "    predicts = list()\n",
    "    for batched_feature in torch.split(fetures, batch_size, dim=0):\n",
    "        batched_predict: torch.Tensor = model(batched_feature.to(device))\n",
    "        predicts.append(batched_predict)\n",
    "    predicts = torch.vstack(predicts)\n",
    "    \n",
    "    predicts[predicts > threshold] = 1\n",
    "    predicts[predicts <= threshold] = 0\n",
    "    fetures = fetures.cpu().detach()\n",
    "    torch.cuda.empty_cache()\n",
    "    return predicts.cpu()\n",
    "\n",
    "def get_binary_weighted_accuracy_row(predicts: torch.Tensor, targets: torch.Tensor):\n",
    "    binary_weighted_accuracy_row = list()\n",
    "    average_bWA = 0\n",
    "    for label_index in range(len(LABELS_NAMES)):\n",
    "        bWA = binary_weighted_accuracy(predicts[:,label_index], targets[:,label_index])\n",
    "        binary_weighted_accuracy_row.append(bWA)\n",
    "        average_bWA += bWA\n",
    "    \n",
    "    average_bWA = average_bWA / len(LABELS_NAMES)\n",
    "    binary_weighted_accuracy_row.append(average_bWA)\n",
    "    \n",
    "    return binary_weighted_accuracy_row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 3)\n",
    "# fig.set_figwidth(20)\n",
    "# fig.set_figheight(15)\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     predicts: torch.Tensor = models[\"Simple FastText Classifier\"](text_features.to(DEVICE)).cpu().detach()\n",
    "# #     text_features = text_features.cpu().detach()\n",
    "# #     torch.cuda.empty_cache()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     predicts: torch.Tensor = models[\"Simple Audio Classifier\"](audio_features.to(DEVICE)).cpu().detach()\n",
    "#     audio_features = audio_features.cpu().detach()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# for label_index in range(len(LABELS_NAMES)):\n",
    "#     thresholds = np.arange(0, 1.0, 0.05)\n",
    "#     bWAs = list()\n",
    "#     for threshold in thresholds:\n",
    "#         tmp_predicts = torch.t_copy(predicts[:,label_index])\n",
    "#         tmp_predicts[tmp_predicts > threshold] = 1\n",
    "#         tmp_predicts[tmp_predicts <= threshold] = 0\n",
    "#         targets = torch.t_copy(predicts[:,label_index])\n",
    "#         bWA = binary_weighted_accuracy(tmp_predicts, targets)\n",
    "#         bWAs.append(bWA)\n",
    "    \n",
    "#     i, j = label_index // 3, label_index % 3\n",
    "#     ax[i][j].plot(thresholds, bWAs, color=\"blue\")\n",
    "#     ax[i][j].set_xlabel(f\"{LABELS_NAMES[label_index]} thresholds\")\n",
    "#     ax[i][j].set_ylabel(\"binary weighted accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA_df = list()\n",
    "models_names = list()\n",
    "\n",
    "# predicts = torch.stack([predict(target, models[\"Stochastic Model\"], 0.5, DEVICE, 1) for target in test_targets])\n",
    "predicts = predict(test_targets, models[\"Stochastic Model\"], 0.5, DEVICE, 1)\n",
    "binary_weighted_accuracy_row = get_binary_weighted_accuracy_row(predicts.to(int), test_targets.to(int))\n",
    "WA_df.append(binary_weighted_accuracy_row)\n",
    "models_names.append(\"Stochastic Model\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for text_model_name in [\"Simple FastText Classifier\", \"Complicated FastText Classifier\"]:\n",
    "    predicts = predict(test_text_features, models[text_model_name], 0.5, DEVICE)\n",
    "    binary_weighted_accuracy_row = get_binary_weighted_accuracy_row(predicts.to(int), test_targets.to(int))\n",
    "    \n",
    "    WA_df.append(binary_weighted_accuracy_row)\n",
    "    models_names.append(text_model_name)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for audio_model_name in [\"Simple Audio Classifier\", \"Complicated Audio Classifier\", \"ResNet\", \"AST\"]:\n",
    "    predicts = predict(test_audio_features, models[audio_model_name], 0.5, DEVICE, 64)\n",
    "    binary_weighted_accuracy_row = get_binary_weighted_accuracy_row(predicts.to(int), test_targets.to(int))\n",
    "    \n",
    "    WA_df.append(binary_weighted_accuracy_row)\n",
    "    models_names.append(audio_model_name)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicts = models[\"Simple Multimodal Classifier\"](test_text_features.to(DEVICE), test_audio_features.to(DEVICE)).cpu()\n",
    "    predicts[predicts > 0.5] = 1\n",
    "    predicts[predicts <= 0.5] = 0\n",
    "    test_text_features = test_text_features.cpu().detach()\n",
    "    test_audio_features = test_audio_features.cpu().detach()\n",
    "    torch.cuda.empty_cache()\n",
    "binary_weighted_accuracy_row = get_binary_weighted_accuracy_row(predicts.to(int), test_targets.to(int))\n",
    "WA_df.append(binary_weighted_accuracy_row)\n",
    "models_names.append(\"Simple Multimodal Classifier\")\n",
    "\n",
    "\n",
    "WA_df = pd.DataFrame(np.asarray(WA_df), columns=LABELS_NAMES+[\"AVERAGE\"], index=models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>surprise</th>\n",
       "      <th>AVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stochastic Model</th>\n",
       "      <td>0.322179</td>\n",
       "      <td>0.345989</td>\n",
       "      <td>0.435221</td>\n",
       "      <td>0.254934</td>\n",
       "      <td>0.304376</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simple FastText Classifier</th>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.415272</td>\n",
       "      <td>0.458601</td>\n",
       "      <td>0.313707</td>\n",
       "      <td>0.375161</td>\n",
       "      <td>0.452166</td>\n",
       "      <td>0.399864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complicated FastText Classifier</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.316924</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.401008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simple Audio Classifier</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.268662</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.392964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complicated Audio Classifier</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.231338</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.386744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResNet</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.307701</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.399471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AST</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.268662</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.392964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simple Multimodal Classifier</th>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.413664</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.313492</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.400436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    anger   disgust      fear     happy  \\\n",
       "Stochastic Model                 0.322179  0.345989  0.435221  0.254934   \n",
       "Simple FastText Classifier       0.384277  0.415272  0.458601  0.313707   \n",
       "Complicated FastText Classifier  0.385135  0.413664  0.458709  0.316924   \n",
       "Simple Audio Classifier          0.385135  0.413664  0.458709  0.268662   \n",
       "Complicated Audio Classifier     0.385135  0.413664  0.458709  0.231338   \n",
       "ResNet                           0.385135  0.413664  0.458709  0.307701   \n",
       "AST                              0.385135  0.413664  0.458709  0.268662   \n",
       "Simple Multimodal Classifier     0.385135  0.413664  0.458709  0.313492   \n",
       "\n",
       "                                      sad  surprise   AVERAGE  \n",
       "Stochastic Model                 0.304376  0.412698  0.345899  \n",
       "Simple FastText Classifier       0.375161  0.452166  0.399864  \n",
       "Complicated FastText Classifier  0.378915  0.452703  0.401008  \n",
       "Simple Audio Classifier          0.378915  0.452703  0.392964  \n",
       "Complicated Audio Classifier     0.378915  0.452703  0.386744  \n",
       "ResNet                           0.378915  0.452703  0.399471  \n",
       "AST                              0.378915  0.452703  0.392964  \n",
       "Simple Multimodal Classifier     0.378915  0.452703  0.400436  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA_df.to_csv(\"WA_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch=3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
